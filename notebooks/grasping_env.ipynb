{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tampura.environment import TampuraEnv\n",
    "from tampura.spec import ProblemSpec\n",
    "from tampura.structs import (\n",
    "    AbstractBelief,\n",
    "    ActionSchema,\n",
    "    AliasStore,\n",
    "    Belief,\n",
    "    NoOp,\n",
    "    Predicate,\n",
    "    State,\n",
    "    effect_from_execute_fn,\n",
    "    Observation\n",
    ")\n",
    "import logging \n",
    "from tampura.symbolic import OBJ, Atom, ForAll\n",
    "from tampura.policies.tampura_policy import TampuraPolicy\n",
    "from tampura.config.config import get_default_config, setup_logger\n",
    "\n",
    "PICK_ONE_SUCCESS = 0.8\n",
    "PICK_BOTH_SUCCESS = 0.5\n",
    "OBJECTS = [f\"{OBJ}o1\", f\"{OBJ}o2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Observation space\n",
    "@dataclass\n",
    "class HoldingObservation(Observation):\n",
    "    holding: List[str] = field(default_factory=lambda: [])\n",
    "\n",
    "# Belief space\n",
    "class HoldingBelief(Belief):\n",
    "    def __init__(self, holding=[]):\n",
    "        self.holding = holding\n",
    "\n",
    "    def update(self, a, o, s):\n",
    "        return HoldingBelief(holding=o.holding)\n",
    "\n",
    "    def abstract(self, store: AliasStore):\n",
    "        return AbstractBelief([Atom(\"holding\", [o]) for o in self.holding])\n",
    "\n",
    "    def vectorize(self):\n",
    "        return np.array([int(obj in self.holding) for obj in OBJECTS])\n",
    "\n",
    "# Action simulators\n",
    "def pick_execute_fn(a, b, s, store):\n",
    "    holding = (\n",
    "        list(set(b.holding + list(a.args))) if random.random() < PICK_ONE_SUCCESS else b.holding\n",
    "    )\n",
    "    return State(), HoldingObservation(holding)\n",
    "\n",
    "\n",
    "def pick_both_execute_fn(a, b, s, store):\n",
    "    holding = (\n",
    "        list(set(b.holding + list(a.args))) if random.random() < PICK_BOTH_SUCCESS else b.holding\n",
    "    )\n",
    "    return State(), HoldingObservation(holding)\n",
    "\n",
    "\n",
    "# Set up environment dynamics\n",
    "class ToyDiscrete(TampuraEnv):\n",
    "    def initialize(self):\n",
    "        store = AliasStore()\n",
    "        for o in OBJECTS:\n",
    "            store.set(o, o, \"physical\")\n",
    "\n",
    "        return HoldingBelief(), store\n",
    "\n",
    "    def get_problem_spec(self) -> ProblemSpec:\n",
    "        predicates = [\n",
    "            Predicate(\"holding\", [\"physical\"]),\n",
    "        ]\n",
    "\n",
    "        action_schemas = [\n",
    "            ActionSchema(\n",
    "                name=\"pick\",\n",
    "                inputs=[\"?o1\"],\n",
    "                input_types=[\"physical\"],\n",
    "                verify_effects=[Atom(\"holding\", [\"?o1\"])],\n",
    "                execute_fn=pick_execute_fn,\n",
    "                effects_fn=effect_from_execute_fn(pick_execute_fn),\n",
    "            ),\n",
    "            ActionSchema(\n",
    "                name=\"pick-both\",\n",
    "                inputs=[\"?o1\", \"?o2\"],\n",
    "                input_types=[\"physical\", \"physical\"],\n",
    "                verify_effects=[Atom(\"holding\", [\"?o1\"]), Atom(\"holding\", [\"?o2\"])],\n",
    "                execute_fn=pick_both_execute_fn,\n",
    "                effects_fn=effect_from_execute_fn(pick_both_execute_fn),\n",
    "            ),\n",
    "            NoOp(),\n",
    "        ]\n",
    "\n",
    "        reward = ForAll(Atom(\"holding\", [\"?o\"]), [\"?o\"], [\"physical\"])\n",
    "\n",
    "        spec = ProblemSpec(\n",
    "            predicates=predicates,\n",
    "            action_schemas=action_schemas,\n",
    "            reward=reward,\n",
    "        )\n",
    "\n",
    "        return spec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment and planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planner\n",
    "cfg = get_default_config(save_dir=os.getcwd())\n",
    "\n",
    "# Set some print options to print out abstract belief, action, observation, and reward\n",
    "cfg[\"print_options\"] = \"ab,a,o,r\"\n",
    "cfg[\"vis_graph\"] = True\n",
    "cfg[\"batch_size\"] = 100\n",
    "cfg[\"num_samples\"] = 100\n",
    "\n",
    "# Initialize environment\n",
    "env = ToyDiscrete(config=cfg)\n",
    "b0, store = env.initialize()\n",
    "\n",
    "# Set up logger to print info\n",
    "setup_logger(cfg[\"save_dir\"], logging.INFO)\n",
    "\n",
    "# Initialize the policy\n",
    "planner = TampuraPolicy(config = cfg, problem_spec = env.problem_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Planner\n",
    "Make sure symk is installed (see README) before running the Tampura planner.\n",
    "With the default settings, the planner should pick both every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = planner.rollout(env, b0, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tampura",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
